{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import optimizers as opt\n",
    "from keras.layers import Input\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from attention.layers import AttentionLayer\n",
    "\n",
    "# Please notice that this dataset is quite simple, so it's easy to overfit a model on it.\n",
    "# We are using it because it comes bundled with Keras and the goal isto showcase the layer only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "MAX_LEN = 100\n",
    "HIDDEN_SIZE = 16\n",
    "DROPOUT = 0.5\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=VOCAB_SIZE)\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, MAX_LEN, padding='post', truncating='post')\n",
    "x_test = sequence.pad_sequences(x_test, MAX_LEN, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    sentence_in = Input((MAX_LEN, ),\n",
    "                        name='sentence-in')\n",
    "\n",
    "    embedded = Embedding(VOCAB_SIZE,\n",
    "                         HIDDEN_SIZE,\n",
    "                         mask_zero=True,\n",
    "                         name='embedding')(sentence_in)\n",
    "\n",
    "    vectors  = LSTM(HIDDEN_SIZE,\n",
    "                    return_sequences=True,\n",
    "                    dropout=DROPOUT,\n",
    "                    recurrent_dropout=DROPOUT,\n",
    "                    name='ff-lstm')(embedded)\n",
    "\n",
    "    sentence = AttentionLayer(name='attention')(vectors)\n",
    "\n",
    "    output   = Dense(1,\n",
    "                     activation='sigmoid',\n",
    "                     name='output')(sentence)\n",
    "\n",
    "    model = models.Model(inputs=[sentence_in], outputs=[output])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sentence-in (InputLayer)     (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 100, 16)           160000    \n",
      "_________________________________________________________________\n",
      "ff-lstm (LSTM)               (None, 100, 16)           2112      \n",
      "_________________________________________________________________\n",
      "attention (AttentionLayer)   (None, 16)                288       \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 162,417\n",
      "Trainable params: 162,417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 74s 4ms/step - loss: 0.6038 - binary_accuracy: 0.6934 - val_loss: 0.4731 - val_binary_accuracy: 0.8058\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 72s 4ms/step - loss: 0.4036 - binary_accuracy: 0.8320 - val_loss: 0.3874 - val_binary_accuracy: 0.8304\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 72s 4ms/step - loss: 0.3253 - binary_accuracy: 0.8678 - val_loss: 0.3684 - val_binary_accuracy: 0.8360\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 72s 4ms/step - loss: 0.2741 - binary_accuracy: 0.8918 - val_loss: 0.3865 - val_binary_accuracy: 0.8364\n",
      "Epoch 5/5\n",
      "14112/20000 [====================>.........] - ETA: 20s - loss: 0.2400 - binary_accuracy: 0.9070"
     ]
    }
   ],
   "source": [
    "# (b, t, d)\n",
    "model = build_model()\n",
    "model.summary()\n",
    "    \n",
    "model.compile('adam', 'binary_crossentropy', metrics=['binary_accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_viz_model(trained_model):\n",
    "    \"\"\"Architecture: input -> embedding -> lstm -> attention -> sigmoid\"\"\"\n",
    "    sentence_in = Input((MAX_LEN, ),\n",
    "                        name='sentence-in')\n",
    "\n",
    "    embedded = Embedding(VOCAB_SIZE,\n",
    "                         HIDDEN_SIZE,\n",
    "                         mask_zero=True,\n",
    "                         weights=trained_model.layers[1].get_weights(),\n",
    "                         name='embedding')(sentence_in)\n",
    "\n",
    "    vectors  = LSTM(HIDDEN_SIZE,\n",
    "                    return_sequences=True,\n",
    "                    dropout=DROPOUT,\n",
    "                    recurrent_dropout=DROPOUT,\n",
    "                    weights=trained_model.layers[2].get_weights(),\n",
    "                    name='ff-lstm')(embedded)\n",
    "\n",
    "    alphas  = AttentionLayer(weights=trained_model.layers[3].get_weights(),\n",
    "                              return_attention=True,\n",
    "                              name='attention')(vectors)\n",
    "\n",
    "    model = models.Model(inputs=[sentence_in], outputs=[alphas])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index2word():\n",
    "    \"\"\"Computes the table that maps ids to their words.\"\"\"\n",
    "    INDEX_FROM = 3   # word index offset\n",
    "\n",
    "    word2index = imdb.get_word_index()\n",
    "    index2word = {v + INDEX_FROM: k for k, v in word2index.items()}\n",
    "    index2word[0] = '[PAD]'\n",
    "    index2word[1] = '[START]'\n",
    "    index2word[2] = '[UNK]'\n",
    "\n",
    "    return index2word\n",
    "\n",
    "\n",
    "def reconstruct(sample, index2word):\n",
    "    \"\"\"Given a list of word ids, returns a list of words.\"\"\"\n",
    "    return [index2word[word] for word in sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _weight2color(brightness):\n",
    "    \"\"\"Converts a single (positive) attention weight to a shade of blue.\"\"\"\n",
    "    brightness = brightness.item()\n",
    "\n",
    "    brightness = int(round(255 * brightness)) # convert from 0.0-1.0 to 0-255\n",
    "    ints = (255 - brightness, 255 - brightness, 255)\n",
    "#     return '#%02x%02x%02x' % ints\n",
    "    return 'rgba({}, {}, {}, 0.6)'.format(*ints)\n",
    "\n",
    "\n",
    "def print_sentence(label, predicted, sentence, weights):\n",
    "    \"\"\"Prints a sample (sequence) making the most attended words background darker.\"\"\"\n",
    "\n",
    "    parts = list()\n",
    "    parts.append('<span style=\"padding:2px;\">[actual: %10s >< pred: %10s]</span> ' % (label, predicted))\n",
    "    for word, weight in zip(sentence, weights):\n",
    "        if word == '[PAD]':\n",
    "            break\n",
    "        parts.append('<span style=\"background: {}; color:#000; padding:2px; font-weight=\\'bold\\'\">{}</span>'.format(_weight2color(weight), word))\n",
    "    \n",
    "    text = ' '.join(parts)\n",
    "    display(Markdown(text))\n",
    "\n",
    "\n",
    "def plot_sentence(words, weights):\n",
    "    words = [f'{i}_{word}' for i, word in enumerate(words)]\n",
    "    \n",
    "    plt.figure(figsize=(20, 2))\n",
    "    plt.plot(words, z)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.grid(alpha=0.4)\n",
    "    plt.ylabel('Attention')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word = get_index2word()\n",
    "index2label = {\n",
    "    0: 'negative',\n",
    "    1: 'positive'\n",
    "}\n",
    "\n",
    "viz_model = build_viz_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word = get_index2word()\n",
    "index2label = {\n",
    "    0: 'negative',\n",
    "    1: 'positive'\n",
    "}\n",
    "\n",
    "for index in range(5):\n",
    "    # reconstructing sample from word ids to actual words\n",
    "    words = reconstruct(x_train[index], index2word)\n",
    "    sample = x_train[index:index+1]\n",
    "\n",
    "    # getting prediction and alphas\n",
    "    pred = int(model.predict(sample)[0] >= 0.5)\n",
    "    z = viz_model.predict(x_train[index:index+1])[0]\n",
    "    \n",
    "    # reescaling for visualization purposes\n",
    "    w = (z - np.min(z)) / (np.max(z) - np.min(z))\n",
    "\n",
    "    # ta-da\n",
    "    print_sentence(index2label[y_train[index]], index2label[pred], words, w)\n",
    "    plot_sentence(words, w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
